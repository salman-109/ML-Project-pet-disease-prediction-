import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split,
RandomizedSearchCV
from sklearn.preprocessing import LabelEncoder, MultiLabelBinarizer
from sklearn.metrics import accuracy_score, classification_report
import xgboost as xgb
import matplotlib.pyplot as plt
from sklearn.model_selection import StratifiedKFold
# 1. Load dataset
df = pd.read_csv("filtered_symptoms_data.csv")
# 2. Define columns
symptom_cols = ['Symptom_1', 'Symptom_2', 'Symptom_3', 'Symptom_4']
label_col = 'Disease'
# 3. Remove duplicates
df = df.drop_duplicates()
# 4. Fill missing symptoms with "None"
df[symptom_cols] = df[symptom_cols].fillna("None")
# 5. Drop rows where all symptoms are "None"
df = df[~(df[symptom_cols] == "None").all(axis=1)]
# 6. Drop rows with missing target
df = df.dropna(subset=[label_col])
# 7. Combine symptoms into a list per row
df['Combined_Symptoms'] = df[symptom_cols].values.tolist()
# 8. One-hot encode symptoms
mlb = MultiLabelBinarizer()
symptom_features =
pd.DataFrame(mlb.fit_transform(df['Combined_Symptoms']),
columns=mlb.classes_)
# 9. Encode target
le = LabelEncoder()
df[label_col] = le.fit_transform(df[label_col])
# 10. Final dataset
df_final = pd.concat([symptom_features,
df[label_col].reset_index(drop=True)], axis=1)
# 11. Split features and labels
X = df_final.drop(label_col, axis=1)
y = df_final[label_col]

X_train, X_test, y_train, y_test = train_test_split(X, y,
test_size=0.3, stratify=y, random_state=42)
# 12. Define XGBoost model
xgb_clf = xgb.XGBClassifier(use_label_encoder=False,
eval_metric='mlogloss', random_state=42)
# 13. Hyperparameter tuning with RandomizedSearchCV
param_dist = {
"n_estimators": [100, 200, 300],
"max_depth": [3, 5, 7, 10],
"learning_rate": [0.01, 0.05, 0.1, 0.2],
"subsample": [0.6, 0.8, 1.0],
"colsample_bytree": [0.6, 0.8, 1.0],
"gamma": [0, 0.1, 0.3],
"reg_alpha": [0, 0.1, 1],
"reg_lambda": [1, 1.5, 2]
}
random_search = RandomizedSearchCV(
estimator=xgb_clf,
param_distributions=param_dist,
n_iter=30,
scoring='accuracy',
cv=3,
verbose=1,
random_state=42,
n_jobs=-1
)
# 14. Fit with eval_set to track training/validation loss
random_search.fit(
X_train, y_train,
eval_set=[(X_train, y_train), (X_test, y_test)],
verbose=False
)
# 15. Evaluate best model
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)
print("✅ Best Parameters:", random_search.best_params_)
print("✅ Accuracy:", accuracy_score(y_test, y_pred))
print("✅ Classification Report:\n", classification_report(y_test,
y_pred, target_names=le.classes_))
# 16. Plot training vs validation log loss
evals_result = best_model.evals_result()
epochs = len(evals_result['validation_0']['mlogloss'])
x_axis = range(0, epochs)

plt.figure(figsize=(10, 6))
plt.plot(x_axis, evals_result['validation_0']['mlogloss'],
label='Train')
plt.plot(x_axis, evals_result['validation_1']['mlogloss'],
label='Validation')
plt.xlabel('Epochs')
plt.ylabel('Log Loss')
plt.title('XGBoost Log Loss (Train vs Validation)')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
Fitting 3 folds for each of 30 candidates, totalling 90 fits
/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158:
UserWarning: [03:43:16] WARNING: /workspace/src/learner.cc:740:
Parameters: { "use_label_encoder" } are not used.
warnings.warn(smsg, UserWarning)
✅ Best Parameters: {'subsample': 1.0, 'reg_lambda': 1, 'reg_alpha':
0.1, 'n_estimators': 300, 'max_depth': 3, 'learning_rate': 0.05,
'gamma': 0.1, 'colsample_bytree': 0.6}
✅ Accuracy: 0.9840232389251997
✅ Classification Report:
precision recall f1-score support
Allergies 1.00 0.99 1.00 117
Cancers 0.96 0.97 0.96 116
Chronic kidney Disease 0.99 0.99 0.99 152
Diabetes 1.00 0.99 1.00 141
Distemper 0.98 0.95 0.97 120
Gastrointestinal Disease 1.00 0.99 1.00 109
Gingitivis 1.00 1.00 1.00 104
Hepatitis 0.96 1.00 0.98 215
Parvovirus 1.00 1.00 1.00 106
Skin Rashes 1.00 1.00 1.00 43
Tetanus 1.00 1.00 1.00 104
Tick fever 0.91 0.86 0.89 50
accuracy 0.98 1377
macro avg 0.98 0.98 0.98 1377
weighted avg 0.98 0.98 0.98 1377

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt
# Predict on test set
y_pred = best_model.predict(X_test)
# Compute confusion matrix
cm = confusion_matrix(y_test, y_pred)
# Display with class labels
disp = ConfusionMatrixDisplay(confusion_matrix=cm,
display_labels=le.classes_)
fig, ax = plt.subplots(figsize=(12, 10))
disp.plot(ax=ax, cmap='Blues', xticks_rotation=45)
plt.title("Confusion Matrix - XGBoost Disease Classifier")
plt.grid(False)
plt.tight_layout()
plt.show()
